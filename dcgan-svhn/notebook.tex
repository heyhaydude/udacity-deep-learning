
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{DCGAN}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \section{Deep Convolutional GANs}\label{deep-convolutional-gans}

In this notebook, you'll build a GAN using convolutional layers in the
generator and discriminator. This is called a Deep Convolutional GAN, or
DCGAN for short. The DCGAN architecture was first explored last year and
has seen impressive results in generating new images, you can read the
\href{https://arxiv.org/pdf/1511.06434.pdf}{original paper here}.

You'll be training DCGAN on the
\href{http://ufldl.stanford.edu/housenumbers/}{Street View House
Numbers} (SVHN) dataset. These are color images of house numbers
collected from Google street view. SVHN images are in color and much
more variable than MNIST.

\begin{figure}
\centering
\includegraphics{assets/SVHN_examples.png}
\caption{SVHN Examples}
\end{figure}

So, we'll need a deeper and more powerful network. This is accomplished
through using convolutional layers in the discriminator and generator.
It's also necessary to use batch normalization to get the convolutional
networks to train. The only real changes compared to what
\href{https://github.com/udacity/deep-learning/tree/master/gan_mnist}{you
saw previously} are in the generator and discriminator, otherwise the
rest of the implementation is the same.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} \PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline
        
        \PY{k+kn}{import} \PY{n+nn}{pickle} \PY{k}{as} \PY{n+nn}{pkl}
        
        \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
        \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
        \PY{k+kn}{from} \PY{n+nn}{scipy}\PY{n+nn}{.}\PY{n+nn}{io} \PY{k}{import} \PY{n}{loadmat}
        \PY{k+kn}{import} \PY{n+nn}{tensorflow} \PY{k}{as} \PY{n+nn}{tf}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{o}{!}mkdir data
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
mkdir: data: File exists

    \end{Verbatim}

    \subsection{Getting the data}\label{getting-the-data}

Here you can download the SVHN dataset. Run the cell above and it'll
download to your machine.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} \PY{k+kn}{from} \PY{n+nn}{urllib}\PY{n+nn}{.}\PY{n+nn}{request} \PY{k}{import} \PY{n}{urlretrieve}
         \PY{k+kn}{from} \PY{n+nn}{os}\PY{n+nn}{.}\PY{n+nn}{path} \PY{k}{import} \PY{n}{isfile}\PY{p}{,} \PY{n}{isdir}
         \PY{k+kn}{from} \PY{n+nn}{tqdm} \PY{k}{import} \PY{n}{tqdm}
         
         \PY{n}{data\PYZus{}dir} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{data/}\PY{l+s+s1}{\PYZsq{}}
         
         \PY{k}{if} \PY{o+ow}{not} \PY{n}{isdir}\PY{p}{(}\PY{n}{data\PYZus{}dir}\PY{p}{)}\PY{p}{:}
             \PY{k}{raise} \PY{n+ne}{Exception}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Data directory doesn}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{t exist!}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         
         \PY{k}{class} \PY{n+nc}{DLProgress}\PY{p}{(}\PY{n}{tqdm}\PY{p}{)}\PY{p}{:}
             \PY{n}{last\PYZus{}block} \PY{o}{=} \PY{l+m+mi}{0}
         
             \PY{k}{def} \PY{n+nf}{hook}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{block\PYZus{}num}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{block\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{total\PYZus{}size}\PY{o}{=}\PY{k+kc}{None}\PY{p}{)}\PY{p}{:}
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{total} \PY{o}{=} \PY{n}{total\PYZus{}size}
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{update}\PY{p}{(}\PY{p}{(}\PY{n}{block\PYZus{}num} \PY{o}{\PYZhy{}} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{last\PYZus{}block}\PY{p}{)} \PY{o}{*} \PY{n}{block\PYZus{}size}\PY{p}{)}
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{last\PYZus{}block} \PY{o}{=} \PY{n}{block\PYZus{}num}
         
         \PY{k}{if} \PY{o+ow}{not} \PY{n}{isfile}\PY{p}{(}\PY{n}{data\PYZus{}dir} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{train\PYZus{}32x32.mat}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{:}
             \PY{k}{with} \PY{n}{DLProgress}\PY{p}{(}\PY{n}{unit}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{B}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{unit\PYZus{}scale}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{miniters}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{desc}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SVHN Training Set}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} \PY{k}{as} \PY{n}{pbar}\PY{p}{:}
                 \PY{n}{urlretrieve}\PY{p}{(}
                     \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{http://ufldl.stanford.edu/housenumbers/train\PYZus{}32x32.mat}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                     \PY{n}{data\PYZus{}dir} \PY{o}{+} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train\PYZus{}32x32.mat}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                     \PY{n}{pbar}\PY{o}{.}\PY{n}{hook}\PY{p}{)}
         
         \PY{k}{if} \PY{o+ow}{not} \PY{n}{isfile}\PY{p}{(}\PY{n}{data\PYZus{}dir} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{test\PYZus{}32x32.mat}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{:}
             \PY{k}{with} \PY{n}{DLProgress}\PY{p}{(}\PY{n}{unit}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{B}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{unit\PYZus{}scale}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{miniters}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{desc}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SVHN Testing Set}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} \PY{k}{as} \PY{n}{pbar}\PY{p}{:}
                 \PY{n}{urlretrieve}\PY{p}{(}
                     \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{http://ufldl.stanford.edu/housenumbers/test\PYZus{}32x32.mat}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                     \PY{n}{data\PYZus{}dir} \PY{o}{+} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test\PYZus{}32x32.mat}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                     \PY{n}{pbar}\PY{o}{.}\PY{n}{hook}\PY{p}{)}
\end{Verbatim}


    These SVHN files are \texttt{.mat} files typically used with Matlab.
However, we can load them in with \texttt{scipy.io.loadmat} which we
imported above.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{n}{trainset} \PY{o}{=} \PY{n}{loadmat}\PY{p}{(}\PY{n}{data\PYZus{}dir} \PY{o}{+} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train\PYZus{}32x32.mat}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{testset} \PY{o}{=} \PY{n}{loadmat}\PY{p}{(}\PY{n}{data\PYZus{}dir} \PY{o}{+} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test\PYZus{}32x32.mat}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    Here I'm showing a small sample of the images. Each of these is 32x32
with 3 color channels (RGB). These are the real images we'll pass to the
discriminator and what the generator will eventually fake.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}13}]:} \PY{n}{idx} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randint}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{trainset}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{,} \PY{n}{size}\PY{o}{=}\PY{l+m+mi}{36}\PY{p}{)}
         \PY{n}{fig}\PY{p}{,} \PY{n}{axes} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{l+m+mi}{6}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{,} \PY{n}{sharex}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{sharey}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{,}\PY{p}{)}
         \PY{k}{for} \PY{n}{ii}\PY{p}{,} \PY{n}{ax} \PY{o+ow}{in} \PY{n+nb}{zip}\PY{p}{(}\PY{n}{idx}\PY{p}{,} \PY{n}{axes}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{:}
             \PY{n}{ax}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{trainset}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{p}{:}\PY{p}{,}\PY{p}{:}\PY{p}{,}\PY{n}{ii}\PY{p}{]}\PY{p}{,} \PY{n}{aspect}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{equal}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{ax}\PY{o}{.}\PY{n}{xaxis}\PY{o}{.}\PY{n}{set\PYZus{}visible}\PY{p}{(}\PY{k+kc}{False}\PY{p}{)}
             \PY{n}{ax}\PY{o}{.}\PY{n}{yaxis}\PY{o}{.}\PY{n}{set\PYZus{}visible}\PY{p}{(}\PY{k+kc}{False}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{subplots\PYZus{}adjust}\PY{p}{(}\PY{n}{wspace}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{hspace}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_8_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Here we need to do a bit of preprocessing and getting the images into a
form where we can pass batches to the network. First off, we need to
rescale the images to a range of -1 to 1, since the output of our
generator is also in that range. We also have a set of test and
validation images which could be used if we're trying to identify the
numbers in the images.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{k}{def} \PY{n+nf}{scale}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{feature\PYZus{}range}\PY{o}{=}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{:}
             \PY{c+c1}{\PYZsh{} scale to (0, 1)}
             \PY{n}{x} \PY{o}{=} \PY{p}{(}\PY{p}{(}\PY{n}{x} \PY{o}{\PYZhy{}} \PY{n}{x}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{o}{/}\PY{p}{(}\PY{l+m+mi}{255} \PY{o}{\PYZhy{}} \PY{n}{x}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{} scale to feature\PYZus{}range}
             \PY{n+nb}{min}\PY{p}{,} \PY{n+nb}{max} \PY{o}{=} \PY{n}{feature\PYZus{}range}
             \PY{n}{x} \PY{o}{=} \PY{n}{x} \PY{o}{*} \PY{p}{(}\PY{n+nb}{max} \PY{o}{\PYZhy{}} \PY{n+nb}{min}\PY{p}{)} \PY{o}{+} \PY{n+nb}{min}
             \PY{k}{return} \PY{n}{x}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}15}]:} \PY{k}{class} \PY{n+nc}{Dataset}\PY{p}{:}
             \PY{k}{def} \PY{n+nf}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{train}\PY{p}{,} \PY{n}{test}\PY{p}{,} \PY{n}{val\PYZus{}frac}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{,} \PY{n}{shuffle}\PY{o}{=}\PY{k+kc}{False}\PY{p}{,} \PY{n}{scale\PYZus{}func}\PY{o}{=}\PY{k+kc}{None}\PY{p}{)}\PY{p}{:}
                 \PY{n}{split\PYZus{}idx} \PY{o}{=} \PY{n+nb}{int}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{test}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{o}{*}\PY{p}{(}\PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n}{val\PYZus{}frac}\PY{p}{)}\PY{p}{)}
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{test\PYZus{}x}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{valid\PYZus{}x} \PY{o}{=} \PY{n}{test}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{p}{:}\PY{p}{,}\PY{p}{:}\PY{p}{,}\PY{p}{:}\PY{n}{split\PYZus{}idx}\PY{p}{]}\PY{p}{,} \PY{n}{test}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{p}{:}\PY{p}{,}\PY{p}{:}\PY{p}{,}\PY{n}{split\PYZus{}idx}\PY{p}{:}\PY{p}{]}
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{test\PYZus{}y}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{valid\PYZus{}y} \PY{o}{=} \PY{n}{test}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{[}\PY{p}{:}\PY{n}{split\PYZus{}idx}\PY{p}{]}\PY{p}{,} \PY{n}{test}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{[}\PY{n}{split\PYZus{}idx}\PY{p}{:}\PY{p}{]}
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{train\PYZus{}x}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{train\PYZus{}y} \PY{o}{=} \PY{n}{train}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{train}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
                 
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{train\PYZus{}x} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{rollaxis}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{train\PYZus{}x}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{valid\PYZus{}x} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{rollaxis}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{valid\PYZus{}x}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{test\PYZus{}x} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{rollaxis}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{test\PYZus{}x}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}
                 
                 \PY{k}{if} \PY{n}{scale\PYZus{}func} \PY{o+ow}{is} \PY{k+kc}{None}\PY{p}{:}
                     \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{scaler} \PY{o}{=} \PY{n}{scale}
                 \PY{k}{else}\PY{p}{:}
                     \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{scaler} \PY{o}{=} \PY{n}{scale\PYZus{}func}
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{shuffle} \PY{o}{=} \PY{n}{shuffle}
                 
             \PY{k}{def} \PY{n+nf}{batches}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{p}{)}\PY{p}{:}
                 \PY{k}{if} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{shuffle}\PY{p}{:}
                     \PY{n}{idx} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{dataset}\PY{o}{.}\PY{n}{train\PYZus{}x}\PY{p}{)}\PY{p}{)}
                     \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{shuffle}\PY{p}{(}\PY{n}{idx}\PY{p}{)}
                     \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{train\PYZus{}x} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{train\PYZus{}x}\PY{p}{[}\PY{n}{idx}\PY{p}{]}
                     \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{train\PYZus{}y} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{train\PYZus{}y}\PY{p}{[}\PY{n}{idx}\PY{p}{]}
                 
                 \PY{n}{n\PYZus{}batches} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{train\PYZus{}y}\PY{p}{)}\PY{o}{/}\PY{o}{/}\PY{n}{batch\PYZus{}size}
                 \PY{k}{for} \PY{n}{ii} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{train\PYZus{}y}\PY{p}{)}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{p}{)}\PY{p}{:}
                     \PY{n}{x} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{train\PYZus{}x}\PY{p}{[}\PY{n}{ii}\PY{p}{:}\PY{n}{ii}\PY{o}{+}\PY{n}{batch\PYZus{}size}\PY{p}{]}
                     \PY{n}{y} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{train\PYZus{}y}\PY{p}{[}\PY{n}{ii}\PY{p}{:}\PY{n}{ii}\PY{o}{+}\PY{n}{batch\PYZus{}size}\PY{p}{]}
                     
                     \PY{k}{yield} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{scaler}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{,} \PY{n}{y}
\end{Verbatim}


    \subsection{Network Inputs}\label{network-inputs}

Here, just creating some placeholders like normal.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}16}]:} \PY{k}{def} \PY{n+nf}{model\PYZus{}inputs}\PY{p}{(}\PY{n}{real\PYZus{}dim}\PY{p}{,} \PY{n}{z\PYZus{}dim}\PY{p}{)}\PY{p}{:}
             \PY{n}{inputs\PYZus{}real} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{placeholder}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{float32}\PY{p}{,} \PY{p}{(}\PY{k+kc}{None}\PY{p}{,} \PY{o}{*}\PY{n}{real\PYZus{}dim}\PY{p}{)}\PY{p}{,} \PY{n}{name}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{input\PYZus{}real}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{inputs\PYZus{}z} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{placeholder}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{float32}\PY{p}{,} \PY{p}{(}\PY{k+kc}{None}\PY{p}{,} \PY{n}{z\PYZus{}dim}\PY{p}{)}\PY{p}{,} \PY{n}{name}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{input\PYZus{}z}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             
             \PY{k}{return} \PY{n}{inputs\PYZus{}real}\PY{p}{,} \PY{n}{inputs\PYZus{}z}
\end{Verbatim}


    \subsection{Generator}\label{generator}

Here you'll build the generator network. The input will be our noise
vector \texttt{z} as before. Also as before, the output will be a
\(tanh\) output, but this time with size 32x32 which is the size of our
SVHN images.

What's new here is we'll use convolutional layers to create our new
images. The first layer is a fully connected layer which is reshaped
into a deep and narrow layer, something like 4x4x1024 as in the original
DCGAN paper. Then we use batch normalization and a leaky ReLU
activation. Next is a transposed convolution where typically you'd halve
the depth and double the width and height of the previous layer. Again,
we use batch normalization and leaky ReLU. For each of these layers, the
general scheme is convolution \textgreater{} batch norm \textgreater{}
leaky ReLU.

You keep stacking layers up like this until you get the final transposed
convolution layer with shape 32x32x3. Below is the archicture used in
the original DCGAN paper:

\begin{figure}
\centering
\includegraphics{assets/dcgan.png}
\caption{DCGAN Generator}
\end{figure}

Note that the final layer here is 64x64x3, while for our SVHN dataset,
we only want it to be 32x32x3.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}17}]:} \PY{k}{def} \PY{n+nf}{generator}\PY{p}{(}\PY{n}{z}\PY{p}{,} \PY{n}{output\PYZus{}dim}\PY{p}{,} \PY{n}{reuse}\PY{o}{=}\PY{k+kc}{False}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{,} \PY{n}{training}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}\PY{p}{:}
             \PY{k}{with} \PY{n}{tf}\PY{o}{.}\PY{n}{variable\PYZus{}scope}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{generator}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{reuse}\PY{o}{=}\PY{n}{reuse}\PY{p}{)}\PY{p}{:}
                 \PY{c+c1}{\PYZsh{} First fully connected layer}
                 \PY{n}{x1} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{dense}\PY{p}{(}\PY{n}{z}\PY{p}{,} \PY{l+m+mi}{4}\PY{o}{*}\PY{l+m+mi}{4}\PY{o}{*}\PY{l+m+mi}{512}\PY{p}{)}
                 \PY{c+c1}{\PYZsh{} Reshape it to start the convolutional stack}
                 \PY{n}{x1} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{x1}\PY{p}{,} \PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{512}\PY{p}{)}\PY{p}{)}
                 \PY{n}{x1} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{batch\PYZus{}normalization}\PY{p}{(}\PY{n}{x1}\PY{p}{,} \PY{n}{training}\PY{o}{=}\PY{n}{training}\PY{p}{)}
                 \PY{n}{x1} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{maximum}\PY{p}{(}\PY{n}{alpha} \PY{o}{*} \PY{n}{x1}\PY{p}{,} \PY{n}{x1}\PY{p}{)}
                 \PY{c+c1}{\PYZsh{} 4x4x512 now}
                 
                 \PY{n}{x2} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{conv2d\PYZus{}transpose}\PY{p}{(}\PY{n}{x1}\PY{p}{,} \PY{l+m+mi}{256}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                 \PY{n}{x2} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{batch\PYZus{}normalization}\PY{p}{(}\PY{n}{x2}\PY{p}{,} \PY{n}{training}\PY{o}{=}\PY{n}{training}\PY{p}{)}
                 \PY{n}{x2} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{maximum}\PY{p}{(}\PY{n}{alpha} \PY{o}{*} \PY{n}{x2}\PY{p}{,} \PY{n}{x2}\PY{p}{)}
                 \PY{c+c1}{\PYZsh{} 8x8x256 now}
                 
                 \PY{n}{x3} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{conv2d\PYZus{}transpose}\PY{p}{(}\PY{n}{x2}\PY{p}{,} \PY{l+m+mi}{128}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                 \PY{n}{x3} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{batch\PYZus{}normalization}\PY{p}{(}\PY{n}{x3}\PY{p}{,} \PY{n}{training}\PY{o}{=}\PY{n}{training}\PY{p}{)}
                 \PY{n}{x3} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{maximum}\PY{p}{(}\PY{n}{alpha} \PY{o}{*} \PY{n}{x3}\PY{p}{,} \PY{n}{x3}\PY{p}{)}
                 \PY{c+c1}{\PYZsh{} 16x16x128 now}
                 
                 \PY{c+c1}{\PYZsh{} Output layer}
                 \PY{n}{logits} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{conv2d\PYZus{}transpose}\PY{p}{(}\PY{n}{x3}\PY{p}{,} \PY{n}{output\PYZus{}dim}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                 \PY{c+c1}{\PYZsh{} 32x32x3 now}
                 
                 \PY{n}{out} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{tanh}\PY{p}{(}\PY{n}{logits}\PY{p}{)}
                 
                 \PY{k}{return} \PY{n}{out}
\end{Verbatim}


    \subsection{Discriminator}\label{discriminator}

Here you'll build the discriminator. This is basically just a
convolutional classifier like you've build before. The input to the
discriminator are 32x32x3 tensors/images. You'll want a few
convolutional layers, then a fully connected layer for the output. As
before, we want a sigmoid output, and you'll need to return the logits
as well. For the depths of the convolutional layers I suggest starting
with 16, 32, 64 filters in the first layer, then double the depth as you
add layers. Note that in the DCGAN paper, they did all the downsampling
using only strided convolutional layers with no maxpool layers.

You'll also want to use batch normalization with
\texttt{tf.layers.batch\_normalization} on each layer except the first
convolutional and output layers. Again, each layer should look something
like convolution \textgreater{} batch norm \textgreater{} leaky ReLU.

Note: in this project, your batch normalization layers will always use
batch statistics. (That is, always set \texttt{training} to
\texttt{True}.) That's because we are only interested in using the
discriminator to help train the generator. However, if you wanted to use
the discriminator for inference later, then you would need to set the
\texttt{training} parameter appropriately.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}18}]:} \PY{k}{def} \PY{n+nf}{discriminator}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{reuse}\PY{o}{=}\PY{k+kc}{False}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{)}\PY{p}{:}
             \PY{k}{with} \PY{n}{tf}\PY{o}{.}\PY{n}{variable\PYZus{}scope}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{discriminator}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{reuse}\PY{o}{=}\PY{n}{reuse}\PY{p}{)}\PY{p}{:}
                 \PY{c+c1}{\PYZsh{} Input layer is 32x32x3}
                 \PY{n}{x1} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{conv2d}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{l+m+mi}{64}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                 \PY{n}{relu1} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{maximum}\PY{p}{(}\PY{n}{alpha} \PY{o}{*} \PY{n}{x1}\PY{p}{,} \PY{n}{x1}\PY{p}{)}
                 \PY{c+c1}{\PYZsh{} 16x16x64}
                 
                 \PY{n}{x2} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{conv2d}\PY{p}{(}\PY{n}{relu1}\PY{p}{,} \PY{l+m+mi}{128}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                 \PY{n}{bn2} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{batch\PYZus{}normalization}\PY{p}{(}\PY{n}{x2}\PY{p}{,} \PY{n}{training}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
                 \PY{n}{relu2} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{maximum}\PY{p}{(}\PY{n}{alpha} \PY{o}{*} \PY{n}{bn2}\PY{p}{,} \PY{n}{bn2}\PY{p}{)}
                 \PY{c+c1}{\PYZsh{} 8x8x128}
                 
                 \PY{n}{x3} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{conv2d}\PY{p}{(}\PY{n}{relu2}\PY{p}{,} \PY{l+m+mi}{256}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                 \PY{n}{bn3} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{batch\PYZus{}normalization}\PY{p}{(}\PY{n}{x3}\PY{p}{,} \PY{n}{training}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
                 \PY{n}{relu3} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{maximum}\PY{p}{(}\PY{n}{alpha} \PY{o}{*} \PY{n}{bn3}\PY{p}{,} \PY{n}{bn3}\PY{p}{)}
                 \PY{c+c1}{\PYZsh{} 4x4x256}
         
                 \PY{c+c1}{\PYZsh{} Flatten it}
                 \PY{n}{flat} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{relu3}\PY{p}{,} \PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{4}\PY{o}{*}\PY{l+m+mi}{4}\PY{o}{*}\PY{l+m+mi}{256}\PY{p}{)}\PY{p}{)}
                 \PY{n}{logits} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{dense}\PY{p}{(}\PY{n}{flat}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
                 \PY{n}{out} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{sigmoid}\PY{p}{(}\PY{n}{logits}\PY{p}{)}
                 
                 \PY{k}{return} \PY{n}{out}\PY{p}{,} \PY{n}{logits}
\end{Verbatim}


    \subsection{Model Loss}\label{model-loss}

Calculating the loss like before, nothing new here.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}19}]:} \PY{k}{def} \PY{n+nf}{model\PYZus{}loss}\PY{p}{(}\PY{n}{input\PYZus{}real}\PY{p}{,} \PY{n}{input\PYZus{}z}\PY{p}{,} \PY{n}{output\PYZus{}dim}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{    Get the loss for the discriminator and generator}
         \PY{l+s+sd}{    :param input\PYZus{}real: Images from the real dataset}
         \PY{l+s+sd}{    :param input\PYZus{}z: Z input}
         \PY{l+s+sd}{    :param out\PYZus{}channel\PYZus{}dim: The number of channels in the output image}
         \PY{l+s+sd}{    :return: A tuple of (discriminator loss, generator loss)}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{n}{g\PYZus{}model} \PY{o}{=} \PY{n}{generator}\PY{p}{(}\PY{n}{input\PYZus{}z}\PY{p}{,} \PY{n}{output\PYZus{}dim}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{n}{alpha}\PY{p}{)}
             \PY{n}{d\PYZus{}model\PYZus{}real}\PY{p}{,} \PY{n}{d\PYZus{}logits\PYZus{}real} \PY{o}{=} \PY{n}{discriminator}\PY{p}{(}\PY{n}{input\PYZus{}real}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{n}{alpha}\PY{p}{)}
             \PY{n}{d\PYZus{}model\PYZus{}fake}\PY{p}{,} \PY{n}{d\PYZus{}logits\PYZus{}fake} \PY{o}{=} \PY{n}{discriminator}\PY{p}{(}\PY{n}{g\PYZus{}model}\PY{p}{,} \PY{n}{reuse}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{n}{alpha}\PY{p}{)}
         
             \PY{n}{d\PYZus{}loss\PYZus{}real} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{reduce\PYZus{}mean}\PY{p}{(}
                 \PY{n}{tf}\PY{o}{.}\PY{n}{nn}\PY{o}{.}\PY{n}{sigmoid\PYZus{}cross\PYZus{}entropy\PYZus{}with\PYZus{}logits}\PY{p}{(}\PY{n}{logits}\PY{o}{=}\PY{n}{d\PYZus{}logits\PYZus{}real}\PY{p}{,} \PY{n}{labels}\PY{o}{=}\PY{n}{tf}\PY{o}{.}\PY{n}{ones\PYZus{}like}\PY{p}{(}\PY{n}{d\PYZus{}model\PYZus{}real}\PY{p}{)}\PY{p}{)}\PY{p}{)}
             \PY{n}{d\PYZus{}loss\PYZus{}fake} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{reduce\PYZus{}mean}\PY{p}{(}
                 \PY{n}{tf}\PY{o}{.}\PY{n}{nn}\PY{o}{.}\PY{n}{sigmoid\PYZus{}cross\PYZus{}entropy\PYZus{}with\PYZus{}logits}\PY{p}{(}\PY{n}{logits}\PY{o}{=}\PY{n}{d\PYZus{}logits\PYZus{}fake}\PY{p}{,} \PY{n}{labels}\PY{o}{=}\PY{n}{tf}\PY{o}{.}\PY{n}{zeros\PYZus{}like}\PY{p}{(}\PY{n}{d\PYZus{}model\PYZus{}fake}\PY{p}{)}\PY{p}{)}\PY{p}{)}
             \PY{n}{g\PYZus{}loss} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{reduce\PYZus{}mean}\PY{p}{(}
                 \PY{n}{tf}\PY{o}{.}\PY{n}{nn}\PY{o}{.}\PY{n}{sigmoid\PYZus{}cross\PYZus{}entropy\PYZus{}with\PYZus{}logits}\PY{p}{(}\PY{n}{logits}\PY{o}{=}\PY{n}{d\PYZus{}logits\PYZus{}fake}\PY{p}{,} \PY{n}{labels}\PY{o}{=}\PY{n}{tf}\PY{o}{.}\PY{n}{ones\PYZus{}like}\PY{p}{(}\PY{n}{d\PYZus{}model\PYZus{}fake}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         
             \PY{n}{d\PYZus{}loss} \PY{o}{=} \PY{n}{d\PYZus{}loss\PYZus{}real} \PY{o}{+} \PY{n}{d\PYZus{}loss\PYZus{}fake}
         
             \PY{k}{return} \PY{n}{d\PYZus{}loss}\PY{p}{,} \PY{n}{g\PYZus{}loss}
\end{Verbatim}


    \subsection{Optimizers}\label{optimizers}

Not much new here, but notice how the train operations are wrapped in a
\texttt{with\ tf.control\_dependencies} block so the batch normalization
layers can update their population statistics.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}20}]:} \PY{k}{def} \PY{n+nf}{model\PYZus{}opt}\PY{p}{(}\PY{n}{d\PYZus{}loss}\PY{p}{,} \PY{n}{g\PYZus{}loss}\PY{p}{,} \PY{n}{learning\PYZus{}rate}\PY{p}{,} \PY{n}{beta1}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{    Get optimization operations}
         \PY{l+s+sd}{    :param d\PYZus{}loss: Discriminator loss Tensor}
         \PY{l+s+sd}{    :param g\PYZus{}loss: Generator loss Tensor}
         \PY{l+s+sd}{    :param learning\PYZus{}rate: Learning Rate Placeholder}
         \PY{l+s+sd}{    :param beta1: The exponential decay rate for the 1st moment in the optimizer}
         \PY{l+s+sd}{    :return: A tuple of (discriminator training operation, generator training operation)}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{c+c1}{\PYZsh{} Get weights and bias to update}
             \PY{n}{t\PYZus{}vars} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{trainable\PYZus{}variables}\PY{p}{(}\PY{p}{)}
             \PY{n}{d\PYZus{}vars} \PY{o}{=} \PY{p}{[}\PY{n}{var} \PY{k}{for} \PY{n}{var} \PY{o+ow}{in} \PY{n}{t\PYZus{}vars} \PY{k}{if} \PY{n}{var}\PY{o}{.}\PY{n}{name}\PY{o}{.}\PY{n}{startswith}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{discriminator}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{]}
             \PY{n}{g\PYZus{}vars} \PY{o}{=} \PY{p}{[}\PY{n}{var} \PY{k}{for} \PY{n}{var} \PY{o+ow}{in} \PY{n}{t\PYZus{}vars} \PY{k}{if} \PY{n}{var}\PY{o}{.}\PY{n}{name}\PY{o}{.}\PY{n}{startswith}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{generator}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{]}
         
             \PY{c+c1}{\PYZsh{} Optimize}
             \PY{k}{with} \PY{n}{tf}\PY{o}{.}\PY{n}{control\PYZus{}dependencies}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{get\PYZus{}collection}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{GraphKeys}\PY{o}{.}\PY{n}{UPDATE\PYZus{}OPS}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                 \PY{n}{d\PYZus{}train\PYZus{}opt} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{train}\PY{o}{.}\PY{n}{AdamOptimizer}\PY{p}{(}\PY{n}{learning\PYZus{}rate}\PY{p}{,} \PY{n}{beta1}\PY{o}{=}\PY{n}{beta1}\PY{p}{)}\PY{o}{.}\PY{n}{minimize}\PY{p}{(}\PY{n}{d\PYZus{}loss}\PY{p}{,} \PY{n}{var\PYZus{}list}\PY{o}{=}\PY{n}{d\PYZus{}vars}\PY{p}{)}
                 \PY{n}{g\PYZus{}train\PYZus{}opt} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{train}\PY{o}{.}\PY{n}{AdamOptimizer}\PY{p}{(}\PY{n}{learning\PYZus{}rate}\PY{p}{,} \PY{n}{beta1}\PY{o}{=}\PY{n}{beta1}\PY{p}{)}\PY{o}{.}\PY{n}{minimize}\PY{p}{(}\PY{n}{g\PYZus{}loss}\PY{p}{,} \PY{n}{var\PYZus{}list}\PY{o}{=}\PY{n}{g\PYZus{}vars}\PY{p}{)}
         
             \PY{k}{return} \PY{n}{d\PYZus{}train\PYZus{}opt}\PY{p}{,} \PY{n}{g\PYZus{}train\PYZus{}opt}
\end{Verbatim}


    \subsection{Building the model}\label{building-the-model}

Here we can use the functions we defined about to build the model as a
class. This will make it easier to move the network around in our code
since the nodes and operations in the graph are packaged in one object.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}21}]:} \PY{k}{class} \PY{n+nc}{GAN}\PY{p}{:}
             \PY{k}{def} \PY{n+nf}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{real\PYZus{}size}\PY{p}{,} \PY{n}{z\PYZus{}size}\PY{p}{,} \PY{n}{learning\PYZus{}rate}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{,} \PY{n}{beta1}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{)}\PY{p}{:}
                 \PY{n}{tf}\PY{o}{.}\PY{n}{reset\PYZus{}default\PYZus{}graph}\PY{p}{(}\PY{p}{)}
                 
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{input\PYZus{}real}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{input\PYZus{}z} \PY{o}{=} \PY{n}{model\PYZus{}inputs}\PY{p}{(}\PY{n}{real\PYZus{}size}\PY{p}{,} \PY{n}{z\PYZus{}size}\PY{p}{)}
                 
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{d\PYZus{}loss}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{g\PYZus{}loss} \PY{o}{=} \PY{n}{model\PYZus{}loss}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{input\PYZus{}real}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{input\PYZus{}z}\PY{p}{,}
                                                       \PY{n}{real\PYZus{}size}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{n}{alpha}\PY{p}{)}
                 
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{d\PYZus{}opt}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{g\PYZus{}opt} \PY{o}{=} \PY{n}{model\PYZus{}opt}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{d\PYZus{}loss}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{g\PYZus{}loss}\PY{p}{,} \PY{n}{learning\PYZus{}rate}\PY{p}{,} \PY{n}{beta1}\PY{p}{)}
\end{Verbatim}


    Here is a function for displaying generated images.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}22}]:} \PY{k}{def} \PY{n+nf}{view\PYZus{}samples}\PY{p}{(}\PY{n}{epoch}\PY{p}{,} \PY{n}{samples}\PY{p}{,} \PY{n}{nrows}\PY{p}{,} \PY{n}{ncols}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{)}\PY{p}{:}
             \PY{n}{fig}\PY{p}{,} \PY{n}{axes} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{n}{figsize}\PY{p}{,} \PY{n}{nrows}\PY{o}{=}\PY{n}{nrows}\PY{p}{,} \PY{n}{ncols}\PY{o}{=}\PY{n}{ncols}\PY{p}{,} 
                                      \PY{n}{sharey}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{sharex}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
             \PY{k}{for} \PY{n}{ax}\PY{p}{,} \PY{n}{img} \PY{o+ow}{in} \PY{n+nb}{zip}\PY{p}{(}\PY{n}{axes}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{samples}\PY{p}{[}\PY{n}{epoch}\PY{p}{]}\PY{p}{)}\PY{p}{:}
                 \PY{n}{ax}\PY{o}{.}\PY{n}{axis}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{off}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                 \PY{n}{img} \PY{o}{=} \PY{p}{(}\PY{p}{(}\PY{n}{img} \PY{o}{\PYZhy{}} \PY{n}{img}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{o}{*}\PY{l+m+mi}{255} \PY{o}{/} \PY{p}{(}\PY{n}{img}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{img}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{uint8}\PY{p}{)}
                 \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}adjustable}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{box\PYZhy{}forced}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                 \PY{n}{im} \PY{o}{=} \PY{n}{ax}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{img}\PY{p}{,} \PY{n}{aspect}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{equal}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
            
             \PY{n}{plt}\PY{o}{.}\PY{n}{subplots\PYZus{}adjust}\PY{p}{(}\PY{n}{wspace}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{hspace}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
             \PY{k}{return} \PY{n}{fig}\PY{p}{,} \PY{n}{axes}
\end{Verbatim}


    And another function we can use to train our network. Notice when we
call \texttt{generator} to create the samples to display, we set
\texttt{training} to \texttt{False}. That's so the batch normalization
layers will use the population statistics rather than the batch
statistics. Also notice that we set the \texttt{net.input\_real}
placeholder when we run the generator's optimizer. The generator doesn't
actually use it, but we'd get an error without it because of the
\texttt{tf.control\_dependencies} block we created in
\texttt{model\_opt}.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}23}]:} \PY{k}{def} \PY{n+nf}{train}\PY{p}{(}\PY{n}{net}\PY{p}{,} \PY{n}{dataset}\PY{p}{,} \PY{n}{epochs}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{p}{,} \PY{n}{print\PYZus{}every}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{show\PYZus{}every}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{)}\PY{p}{:}
             \PY{n}{saver} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{train}\PY{o}{.}\PY{n}{Saver}\PY{p}{(}\PY{p}{)}
             \PY{n}{sample\PYZus{}z} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{uniform}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{72}\PY{p}{,} \PY{n}{z\PYZus{}size}\PY{p}{)}\PY{p}{)}
         
             \PY{n}{samples}\PY{p}{,} \PY{n}{losses} \PY{o}{=} \PY{p}{[}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{p}{]}
             \PY{n}{steps} \PY{o}{=} \PY{l+m+mi}{0}
         
             \PY{k}{with} \PY{n}{tf}\PY{o}{.}\PY{n}{Session}\PY{p}{(}\PY{p}{)} \PY{k}{as} \PY{n}{sess}\PY{p}{:}
                 \PY{n}{sess}\PY{o}{.}\PY{n}{run}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{global\PYZus{}variables\PYZus{}initializer}\PY{p}{(}\PY{p}{)}\PY{p}{)}
                 \PY{k}{for} \PY{n}{e} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{epochs}\PY{p}{)}\PY{p}{:}
                     \PY{k}{for} \PY{n}{x}\PY{p}{,} \PY{n}{y} \PY{o+ow}{in} \PY{n}{dataset}\PY{o}{.}\PY{n}{batches}\PY{p}{(}\PY{n}{batch\PYZus{}size}\PY{p}{)}\PY{p}{:}
                         \PY{n}{steps} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
         
                         \PY{c+c1}{\PYZsh{} Sample random noise for G}
                         \PY{n}{batch\PYZus{}z} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{uniform}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{size}\PY{o}{=}\PY{p}{(}\PY{n}{batch\PYZus{}size}\PY{p}{,} \PY{n}{z\PYZus{}size}\PY{p}{)}\PY{p}{)}
         
                         \PY{c+c1}{\PYZsh{} Run optimizers}
                         \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{sess}\PY{o}{.}\PY{n}{run}\PY{p}{(}\PY{n}{net}\PY{o}{.}\PY{n}{d\PYZus{}opt}\PY{p}{,} \PY{n}{feed\PYZus{}dict}\PY{o}{=}\PY{p}{\PYZob{}}\PY{n}{net}\PY{o}{.}\PY{n}{input\PYZus{}real}\PY{p}{:} \PY{n}{x}\PY{p}{,} \PY{n}{net}\PY{o}{.}\PY{n}{input\PYZus{}z}\PY{p}{:} \PY{n}{batch\PYZus{}z}\PY{p}{\PYZcb{}}\PY{p}{)}
                         \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{sess}\PY{o}{.}\PY{n}{run}\PY{p}{(}\PY{n}{net}\PY{o}{.}\PY{n}{g\PYZus{}opt}\PY{p}{,} \PY{n}{feed\PYZus{}dict}\PY{o}{=}\PY{p}{\PYZob{}}\PY{n}{net}\PY{o}{.}\PY{n}{input\PYZus{}z}\PY{p}{:} \PY{n}{batch\PYZus{}z}\PY{p}{,} \PY{n}{net}\PY{o}{.}\PY{n}{input\PYZus{}real}\PY{p}{:} \PY{n}{x}\PY{p}{\PYZcb{}}\PY{p}{)}
         
                         \PY{k}{if} \PY{n}{steps} \PY{o}{\PYZpc{}} \PY{n}{print\PYZus{}every} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
                             \PY{c+c1}{\PYZsh{} At the end of each epoch, get the losses and print them out}
                             \PY{n}{train\PYZus{}loss\PYZus{}d} \PY{o}{=} \PY{n}{net}\PY{o}{.}\PY{n}{d\PYZus{}loss}\PY{o}{.}\PY{n}{eval}\PY{p}{(}\PY{p}{\PYZob{}}\PY{n}{net}\PY{o}{.}\PY{n}{input\PYZus{}z}\PY{p}{:} \PY{n}{batch\PYZus{}z}\PY{p}{,} \PY{n}{net}\PY{o}{.}\PY{n}{input\PYZus{}real}\PY{p}{:} \PY{n}{x}\PY{p}{\PYZcb{}}\PY{p}{)}
                             \PY{n}{train\PYZus{}loss\PYZus{}g} \PY{o}{=} \PY{n}{net}\PY{o}{.}\PY{n}{g\PYZus{}loss}\PY{o}{.}\PY{n}{eval}\PY{p}{(}\PY{p}{\PYZob{}}\PY{n}{net}\PY{o}{.}\PY{n}{input\PYZus{}z}\PY{p}{:} \PY{n}{batch\PYZus{}z}\PY{p}{\PYZcb{}}\PY{p}{)}
         
                             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Epoch }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{/}\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{...}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{e}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{epochs}\PY{p}{)}\PY{p}{,}
                                   \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Discriminator Loss: }\PY{l+s+si}{\PYZob{}:.4f\PYZcb{}}\PY{l+s+s2}{...}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{train\PYZus{}loss\PYZus{}d}\PY{p}{)}\PY{p}{,}
                                   \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Generator Loss: }\PY{l+s+si}{\PYZob{}:.4f\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{train\PYZus{}loss\PYZus{}g}\PY{p}{)}\PY{p}{)}
                             \PY{c+c1}{\PYZsh{} Save losses to view after training}
                             \PY{n}{losses}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{p}{(}\PY{n}{train\PYZus{}loss\PYZus{}d}\PY{p}{,} \PY{n}{train\PYZus{}loss\PYZus{}g}\PY{p}{)}\PY{p}{)}
         
                         \PY{k}{if} \PY{n}{steps} \PY{o}{\PYZpc{}} \PY{n}{show\PYZus{}every} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
                             \PY{n}{gen\PYZus{}samples} \PY{o}{=} \PY{n}{sess}\PY{o}{.}\PY{n}{run}\PY{p}{(}
                                            \PY{n}{generator}\PY{p}{(}\PY{n}{net}\PY{o}{.}\PY{n}{input\PYZus{}z}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{n}{reuse}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{training}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{p}{,}
                                            \PY{n}{feed\PYZus{}dict}\PY{o}{=}\PY{p}{\PYZob{}}\PY{n}{net}\PY{o}{.}\PY{n}{input\PYZus{}z}\PY{p}{:} \PY{n}{sample\PYZus{}z}\PY{p}{\PYZcb{}}\PY{p}{)}
                             \PY{n}{samples}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{gen\PYZus{}samples}\PY{p}{)}
                             \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{view\PYZus{}samples}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{samples}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{,} \PY{l+m+mi}{12}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{n}{figsize}\PY{p}{)}
                             \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
         
                 \PY{n}{saver}\PY{o}{.}\PY{n}{save}\PY{p}{(}\PY{n}{sess}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./checkpoints/generator.ckpt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
             \PY{k}{with} \PY{n+nb}{open}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{samples.pkl}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{wb}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} \PY{k}{as} \PY{n}{f}\PY{p}{:}
                 \PY{n}{pkl}\PY{o}{.}\PY{n}{dump}\PY{p}{(}\PY{n}{samples}\PY{p}{,} \PY{n}{f}\PY{p}{)}
             
             \PY{k}{return} \PY{n}{losses}\PY{p}{,} \PY{n}{samples}
\end{Verbatim}


    \subsection{Hyperparameters}\label{hyperparameters}

GANs are very sensitive to hyperparameters. A lot of experimentation
goes into finding the best hyperparameters such that the generator and
discriminator don't overpower each other. Try out your own
hyperparameters or read \href{https://arxiv.org/pdf/1511.06434.pdf}{the
DCGAN paper} to see what worked for them.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{n}{real\PYZus{}size} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{32}\PY{p}{,}\PY{l+m+mi}{32}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}
        \PY{n}{z\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{100}
        \PY{n}{learning\PYZus{}rate} \PY{o}{=} \PY{l+m+mf}{0.0002}
        \PY{n}{batch\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{128}
        \PY{n}{epochs} \PY{o}{=} \PY{l+m+mi}{25}
        \PY{n}{alpha} \PY{o}{=} \PY{l+m+mf}{0.2}
        \PY{n}{beta1} \PY{o}{=} \PY{l+m+mf}{0.5}
        
        \PY{c+c1}{\PYZsh{} Create the network}
        \PY{n}{net} \PY{o}{=} \PY{n}{GAN}\PY{p}{(}\PY{n}{real\PYZus{}size}\PY{p}{,} \PY{n}{z\PYZus{}size}\PY{p}{,} \PY{n}{learning\PYZus{}rate}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{n}{alpha}\PY{p}{,} \PY{n}{beta1}\PY{o}{=}\PY{n}{beta1}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{n}{dataset} \PY{o}{=} \PY{n}{Dataset}\PY{p}{(}\PY{n}{trainset}\PY{p}{,} \PY{n}{testset}\PY{p}{)}
        
        \PY{n}{losses}\PY{p}{,} \PY{n}{samples} \PY{o}{=} \PY{n}{train}\PY{p}{(}\PY{n}{net}\PY{p}{,} \PY{n}{dataset}\PY{p}{,} \PY{n}{epochs}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Epoch 1/25{\ldots} Discriminator Loss: 1.2580{\ldots} Generator Loss: 0.7465
Epoch 1/25{\ldots} Discriminator Loss: 0.4731{\ldots} Generator Loss: 1.4121
Epoch 1/25{\ldots} Discriminator Loss: 0.1657{\ldots} Generator Loss: 2.4495
Epoch 1/25{\ldots} Discriminator Loss: 0.1071{\ldots} Generator Loss: 3.4634
Epoch 1/25{\ldots} Discriminator Loss: 0.1291{\ldots} Generator Loss: 2.9409
Epoch 1/25{\ldots} Discriminator Loss: 0.1143{\ldots} Generator Loss: 3.0612
Epoch 1/25{\ldots} Discriminator Loss: 0.1084{\ldots} Generator Loss: 3.0190
Epoch 1/25{\ldots} Discriminator Loss: 0.2760{\ldots} Generator Loss: 2.7519
Epoch 1/25{\ldots} Discriminator Loss: 0.1104{\ldots} Generator Loss: 3.3026
Epoch 1/25{\ldots} Discriminator Loss: 0.2571{\ldots} Generator Loss: 3.6032

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_30_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
Epoch 1/25{\ldots} Discriminator Loss: 0.3109{\ldots} Generator Loss: 2.6759
Epoch 1/25{\ldots} Discriminator Loss: 0.8838{\ldots} Generator Loss: 1.3977
Epoch 1/25{\ldots} Discriminator Loss: 0.7081{\ldots} Generator Loss: 1.2678
Epoch 1/25{\ldots} Discriminator Loss: 0.5001{\ldots} Generator Loss: 1.5559
Epoch 1/25{\ldots} Discriminator Loss: 0.8936{\ldots} Generator Loss: 0.7488
Epoch 1/25{\ldots} Discriminator Loss: 0.5480{\ldots} Generator Loss: 1.4427
Epoch 1/25{\ldots} Discriminator Loss: 0.4496{\ldots} Generator Loss: 1.3099
Epoch 1/25{\ldots} Discriminator Loss: 0.2423{\ldots} Generator Loss: 2.3265
Epoch 1/25{\ldots} Discriminator Loss: 1.2811{\ldots} Generator Loss: 0.5301
Epoch 1/25{\ldots} Discriminator Loss: 2.2911{\ldots} Generator Loss: 3.2147

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_30_3.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
Epoch 1/25{\ldots} Discriminator Loss: 0.3162{\ldots} Generator Loss: 1.9110
Epoch 1/25{\ldots} Discriminator Loss: 0.2661{\ldots} Generator Loss: 4.0425
Epoch 1/25{\ldots} Discriminator Loss: 2.6188{\ldots} Generator Loss: 0.3231
Epoch 1/25{\ldots} Discriminator Loss: 1.4519{\ldots} Generator Loss: 0.8121
Epoch 1/25{\ldots} Discriminator Loss: 0.5683{\ldots} Generator Loss: 2.7232
Epoch 1/25{\ldots} Discriminator Loss: 0.5634{\ldots} Generator Loss: 1.2740
Epoch 1/25{\ldots} Discriminator Loss: 1.0189{\ldots} Generator Loss: 1.1699
Epoch 1/25{\ldots} Discriminator Loss: 1.0587{\ldots} Generator Loss: 0.7801
Epoch 1/25{\ldots} Discriminator Loss: 0.7107{\ldots} Generator Loss: 1.3087
Epoch 1/25{\ldots} Discriminator Loss: 0.4219{\ldots} Generator Loss: 2.3637

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_30_5.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
Epoch 1/25{\ldots} Discriminator Loss: 0.4517{\ldots} Generator Loss: 1.9842
Epoch 1/25{\ldots} Discriminator Loss: 0.2648{\ldots} Generator Loss: 2.3929
Epoch 1/25{\ldots} Discriminator Loss: 0.4842{\ldots} Generator Loss: 4.1248
Epoch 1/25{\ldots} Discriminator Loss: 0.4407{\ldots} Generator Loss: 2.1259
Epoch 1/25{\ldots} Discriminator Loss: 0.3989{\ldots} Generator Loss: 1.8731
Epoch 1/25{\ldots} Discriminator Loss: 0.6292{\ldots} Generator Loss: 1.1105
Epoch 1/25{\ldots} Discriminator Loss: 0.4879{\ldots} Generator Loss: 1.4286
Epoch 1/25{\ldots} Discriminator Loss: 0.4672{\ldots} Generator Loss: 2.1989

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}18}]:} \PY{n}{fig}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{p}{)}
         \PY{n}{losses} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{losses}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{losses}\PY{o}{.}\PY{n}{T}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Discriminator}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{losses}\PY{o}{.}\PY{n}{T}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Generator}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Training Losses}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}18}]:} <matplotlib.legend.Legend at 0x7f2c43f318d0>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_31_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}18}]:} \PY{n}{fig}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{p}{)}
         \PY{n}{losses} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{losses}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{losses}\PY{o}{.}\PY{n}{T}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Discriminator}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{losses}\PY{o}{.}\PY{n}{T}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Generator}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Training Losses}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}18}]:} <matplotlib.legend.Legend at 0x7f785c9e7a58>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_32_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}19}]:} \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{view\PYZus{}samples}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{samples}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{,} \PY{l+m+mi}{12}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_33_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}19}]:} \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{view\PYZus{}samples}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{samples}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{,} \PY{l+m+mi}{12}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_34_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    

    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
